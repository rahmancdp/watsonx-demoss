import sys
import logging
import os
import tempfile
import pathlib
import time

import streamlit as st
from dotenv import load_dotenv

from langchain.document_loaders import PyPDFLoader
from sentence_transformers import SentenceTransformer

# from genai.credentials import Credentials
# from genai.schemas import GenerateParams
# from genai.model import Model

from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes
from ibm_watson_machine_learning.foundation_models import Model
from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams
from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM


from typing import Literal, Optional, Any
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma, FAISS
from langchain.embeddings import HuggingFaceEmbeddings
import numpy as np
# Most GENAI logs are at Debug level.
# logging.basicConfig(level=os.environ.get("LOGLEVEL", "DEBUG"))

st.set_page_config(
    page_title="æŠ€æœ¯æ”¯æŒ",
    page_icon="ğŸ§Š",
    layout="wide",
    initial_sidebar_state="expanded"
)
st.markdown("""
    <style>
        .reportview-container {
            margin-top: -2em;
        }
        #MainMenu {visibility: hidden;}
        .stDeployButton {display:none;}
        footer {visibility: hidden;}
        #stDecoration {display:none;}
    </style>
""", unsafe_allow_html=True)

hide_streamlit_style = """
                <style>
                div[data-testid="stToolbar"] {
                visibility: hidden;
                height: 0%;
                position: fixed;
                }
                div[data-testid="stDecoration"] {
                visibility: hidden;
                height: 0%;
                position: fixed;
                }
                div[data-testid="stStatusWidget"] {
                visibility: hidden;
                height: 0%;
                position: fixed;
                }
                #MainMenu {
                visibility: hidden;
                height: 0%;
                }
                header {
                visibility: hidden;
                height: 0%;
                }
                footer {
                visibility: hidden;
                height: 0%;
                }
                </style>
                """
st.markdown(hide_streamlit_style, unsafe_allow_html=True) 


st.header("ç¨…æ³•æŸ¥è©¢")
# chunk_size=1500
# chunk_overlap = 200

load_dotenv()

api_key = os.getenv("API_KEY", None)
project_id = os.getenv("PROJECT_ID", None)

# handler = StdOutCallbackHandler()

creds = {
    "url"    : "https://us-south.ml.cloud.ibm.com",
    "apikey" : api_key
}

params = {
    GenParams.DECODING_METHOD:"greedy",
    GenParams.MAX_NEW_TOKENS:1000,
    GenParams.MIN_NEW_TOKENS:1,
    GenParams.TEMPERATURE:0.5,
    GenParams.TOP_K:50,
    GenParams.TOP_P:1
}

@st.cache_data
def read_pdf(uploaded_files,chunk_size =250,chunk_overlap=20):
    docs = []
    for uploaded_file in uploaded_files:
        bytes_data = uploaded_file.read()
        with tempfile.NamedTemporaryFile(mode='wb', delete=False) as temp_file:
        # Write content to the temporary file
            temp_file.write(bytes_data)
            filepath = temp_file.name
            with st.spinner('æ­£åœ¨ä¸Šå‚³PDF'):
                loader = PyPDFLoader(filepath)
                data = loader.load()
                text_splitter = RecursiveCharacterTextSplitter(chunk_size= chunk_size, chunk_overlap=chunk_overlap)
                docs += text_splitter.split_documents(data)
    return docs

def read_push_embeddings(docs):
    embeddings = HuggingFaceEmbeddings(model_name="paraphrase-multilingual-MiniLM-L12-v2")
    # embeddings = HuggingFaceEmbeddings()
    temp_dir = tempfile.TemporaryDirectory()
    db = FAISS.from_documents(docs, embeddings)
    return db

def querypdf(informations, history, question):
    # prompt = f"""
    # answer the question in 5 sentences base on the informations:
    # informations:
    # {informations}
    # question:
    # {question}
    # answer in point form:"""

    prompt = f"""[INST]ä½œç‚ºä¸€ä½ç¨…æ³•å°ˆå®¶ï¼Œè«‹æ ¹æ“šæä¾›çš„ç¨…æ³•æ–‡ä»¶ç”¨ä¸­æ–‡å›ç­”ã€‚
    -è«‹åªæ ¹æ“šæä¾›çš„ç¨…æ³•æ–‡ä»¶å›ç­”ï¼Œèªªæ˜ä¿¡æ¯ä¾†æºã€‚
    -è«‹åªç”¨ä¸­æ–‡å›ç­”ï¼Œä¸è¦å‡ºç¾äº‚ç¢¼ã€‚
    -å°æ–¼éœ€è¦è¨ˆç®—çš„å…§å®¹ï¼Œè«‹å°å¿ƒä¸€æ­¥æ­¥è¨ˆç®—ä½œç­”ï¼Œå¦å‰‡åªæä¾›ç®—å¼å°±å¥½ï¼Œä¸è¦å¹«åŠ©è²´è³“è¨ˆç®—ã€‚
    -ä¸€æ­¥æ­¥è§£ç­”ã€‚
    -å¦‚æœä¸çŸ¥é“ï¼Œä¸è¦çŒœæ¸¬ï¼Œå°±èªªä¸çŸ¥é“ï¼Œä¸¦è«‹è²´è³“æŸ¥çœ‹ç¶²ç«™ä¿¡æ¯ã€‚
    <<SYS>>
    ç¨…æ³•æ–‡ä»¶:
    {informations}
    å‰é¢çš„æŸ¥è©¢:
    {history}
    <<SYS>>
    æå•:
    {question}
    [/INST]
    è§£ç­”:"""

    prompts = [prompt]
    answer = ""
    for response in model.generate_text(prompts):
        answer += response.replace("\\n\\n","\n")
    return answer

model = Model("meta-llama/llama-2-13b-chat",creds, params, project_id)

history = []

if "db" not in st.session_state:
    st.session_state.db = None

# Sidebar contents
with st.sidebar:
    st.title("ç¨…æ³•æŸ¥è©¢")
    uploaded_files = st.file_uploader("ä¸Šä¼ ä¸€ä¸ªPDFæ–‡æ¡£", accept_multiple_files=True)
    if st.session_state.db is None:
        starttime = time.time()

        docs = read_pdf(uploaded_files)
        if docs is not None and len(docs) > 0:
            st.session_state.db = read_push_embeddings(docs)

        endtime = time.time()
        print(f"take {endtime-starttime} to ingest the doc to vectordb")

with st.chat_message("system"):
    st.write("è«‹è¼¸å…¥ä½ çš„æŸ¥è©¢")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if query := st.chat_input("your query"):
    with st.chat_message("user"):
        st.markdown(query)

    history += [query]

    st.session_state.messages.append({"role": "user", "content": query})
    with st.spinner(text="æ­£åœ¨æŸ¥è©¢ä¸­...", cache=False):
        starttime = time.time()
        docs = st.session_state.db.similarity_search(query)
        endtime = time.time()
        print(f"take {endtime-starttime} to search similary")
        starttime = time.time()
        answer = querypdf(docs, history, query)
        endtime = time.time() 
        print(f"take {endtime-starttime} to build the answer")

    st.session_state.messages.append({"role": "agent", "content": answer}) 

    with st.chat_message("agent"):
        st.markdown(answer)